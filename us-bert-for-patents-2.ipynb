{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nGuide: Baseline Guide<br>\nInference: USPPPM: DeBERTa V3 Small [Inference]","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:10:53.782950Z","iopub.execute_input":"2022-06-20T14:10:53.783249Z","iopub.status.idle":"2022-06-20T14:11:01.190418Z","shell.execute_reply.started":"2022-06-20T14:10:53.783214Z","shell.execute_reply":"2022-06-20T14:11:01.189543Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.16.2)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.49)\nRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.11.6)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/torch-components-library/torch-components-main\")\nsys.path.append(\"../input/transformers/src\")\nsys.path.append(\"../input/mixout-github-code/mixout\")\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.optim import lr_scheduler\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.checkpoint import checkpoint\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom torch_components import Configuration as Config, Timer, Averager\nfrom torch_components.callbacks import EarlyStopping, ModelCheckpoint\nfrom torch_components.utils import seed_everything, get_lr, get_optimizer, get_scheduler\nfrom torch_components.import_utils import wandb_run_exists\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom mixout import MixLinear, Mixout\nfrom tqdm.notebook import tqdm\nfrom IPython.display import display\nfrom datetime import timedelta\nimport scipy\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport wandb\nimport os\nimport shutil\nimport gc\nfrom kaggle_secrets import UserSecretsClient\n\n\nos.environ[\"EXPERIMENT_NAME\"] = \"anferico/bert-for-patents\"\n\nEXPERIMENT_NAME = os.environ.get(\"EXPERIMENT_NAME\")\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nWANDB = False\nDEBUG = True\nUSER_SECRETS = UserSecretsClient()\n\n\nif WANDB:\n    os.environ[\"WANDB_PROJECT\"] = \"uspppm\"\n    os.environ[\"WANDB_ENTITY\"] = \"uspppm\"\n    os.environ[\"WANDB_SILENT\"] = \"true\"\n    \n    wandb_secret_name = \"wandb_api_key\"\n    wandb_key = USER_SECRETS.get_secret(wandb_secret_name)\n    \n    EXPERIMENT_NAME = EXPERIMENT_NAME if EXPERIMENT_NAME != \"none\" else wandb.util.generate_id()\n    wandb.login(key=wandb_key)\n    \nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\nwarnings.simplefilter(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-20T14:11:01.192403Z","iopub.execute_input":"2022-06-20T14:11:01.192624Z","iopub.status.idle":"2022-06-20T14:11:01.204853Z","shell.execute_reply.started":"2022-06-20T14:11:01.192595Z","shell.execute_reply":"2022-06-20T14:11:01.204160Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"config = Config(model=dict(model_path=\"anferico/bert-for-patents\"),\n                optimizer=dict(name=\"AdamW\", parameters=dict(lr=1e-5, weight_decay=0.01)),\n                scheduler=dict(name=\"get_cosine_with_hard_restarts_schedule_with_warmup\", \n                               parameters=dict(num_cycles=2, last_epoch=-1)),\n                warmup=0.1,\n                scheduling_after=\"step\",\n                seed=180,\n                max_length=500,\n                batch_size=80,\n                epochs=5,\n                num_workers=4,\n                pin_memory=True,\n                folds=4,\n                validation_steps=200, \n                gradient_accumulation_steps=1,\n                gradient_norm=1.0,\n                gradient_scaling=True,\n                delta=1e-4,\n                verbose=100,\n                save_model=False,\n                device=DEVICE,\n                input_directory=\"./\",\n                output_directory=\"./\",\n                cv_monitor_value=\"pearson\",\n                amp=True, \n                debug=True,\n                decimals=4)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:01.206557Z","iopub.execute_input":"2022-06-20T14:11:01.206976Z","iopub.status.idle":"2022-06-20T14:11:01.217954Z","shell.execute_reply.started":"2022-06-20T14:11:01.206939Z","shell.execute_reply":"2022-06-20T14:11:01.217302Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"config.seed = seed_everything(config.seed)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:01.219796Z","iopub.execute_input":"2022-06-20T14:11:01.220411Z","iopub.status.idle":"2022-06-20T14:11:01.229956Z","shell.execute_reply.started":"2022-06-20T14:11:01.220368Z","shell.execute_reply":"2022-06-20T14:11:01.229096Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"def make_directory(directory, overwriting=False):\n    if not os.path.exists(directory):\n        os.mkdir(directory)\n    else:\n        if overwriting:\n            shutil.rmtree(directory)\n            os.mkdir(directory)\n\n            \ndef create_folds(data_frame, targets, groups, folds=4, seed=42, shuffle=True, fold_column=\"fold\"):\n    cv_strategy = StratifiedGroupKFold(n_splits=folds, random_state=seed, shuffle=shuffle)\n    folds = cv_strategy.split(X=data_frame, y=targets, groups=groups)\n    for fold, (train_indexes, validation_indexes) in enumerate(folds):\n        data_frame.loc[validation_indexes, fold_column] =  int(fold+1)\n        \n    data_frame[fold_column] = data_frame[fold_column].astype(int)\n    \n    return data_frame","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:01.232322Z","iopub.execute_input":"2022-06-20T14:11:01.232766Z","iopub.status.idle":"2022-06-20T14:11:01.241055Z","shell.execute_reply.started":"2022-06-20T14:11:01.232726Z","shell.execute_reply":"2022-06-20T14:11:01.240239Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def training_loop(train_loader, \n                  model,\n                  optimizer,\n                  scheduler=None,\n                  scheduling_after=\"step\",\n                  epochs=1,\n                  validation_loader=None, \n                  gradient_accumulation_steps=1, \n                  gradient_scaling=False,\n                  gradient_norm=1,\n                  validation_steps=\"epoch\", \n                  amp=False,\n                  recalculate_metrics_at_end=True, \n                  return_validation_outputs=True,\n                  debug=True, \n                  teacher_model=None,\n                  pseudo_loader=None,\n                  verbose=100, \n                  device=\"cpu\", \n                  time_format=\"{hours}:{minutes}:{seconds}\", \n                  logger=[\"print\", \"wandb\"], \n                  decimals=4):\n    \n    training_steps = len(train_loader) * epochs\n    \n    if isinstance(validation_steps, float):\n        validation_steps = int(training_steps * validation_steps)\n    elif validation_steps == \"epoch\":\n        validation_steps = len(train_loader)\n    \n    if debug:\n        print(f\"Epochs: {epochs}\")\n        print(f\"Auto Mixed Precision: {amp}\")\n        print(f\"Gradient norm: {gradient_norm}\")\n        print(f\"Gradient scaling: {gradient_scaling}\")\n        print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n        print(f\"Validation steps: {validation_steps}\")\n        print(f\"Device: {device}\")\n        print()\n        \n    if wandb_run_exists() and \"wandb\" in logger:\n        print(f\"Weights & Biases Run: {wandb.run.get_url()}\", end=\"\\n\"*2)\n        \n    passed_steps = 1\n    train_loss, train_metrics = Averager(), Averager()\n    scaler = GradScaler() if gradient_scaling else None\n    best_validation_loss, best_validation_metrics, best_validation_outputs = None, None, None\n    total_time = timedelta(seconds=0)\n    \n    if device is not None: \n        model.to(device)\n        \n        if teacher_model is not None: teacher_model.to(device)\n    \n    for epoch in range(1, epochs+1):\n        if \"tqdm\" in logger:\n            bar_format = \"{l_bar} {bar} {n_fmt}/{total_fmt} - remain: {remaining}{postfix}\"\n            train_loader = tqdm(iterable=train_loader, \n                                total=len(train_loader),\n                                colour=\"#000\",\n                                bar_format=bar_format)\n            \n            train_loader.set_description_str(f\"Epoch {epoch}/{epochs}\")\n        \n        if \"print\" in logger:\n            print(f\"\\nEpoch {epoch}/{epochs}\", end=\"\\n\"*2)\n            \n        epoch_train_loss, epoch_train_metrics = Averager(), Averager()\n        timer = Timer(time_format)\n        steps = len(train_loader)    \n        \n        model.zero_grad()\n        for step, batch in enumerate(train_loader, 1):\n            batch_size = train_loader.batch_size\n            \n            step_timer =  Timer(time_format)\n            pseudo_batch = next(iter(pseudo_loader)) if pseudo_loader is not None else None\n            batch_loss, batch_metrics = training_step(batch=batch, \n                                                      model=model, \n                                                      optimizer=optimizer,\n                                                      gradient_norm=gradient_norm,\n                                                      gradient_accumulation_steps=gradient_accumulation_steps, \n                                                      amp=amp, \n                                                      scaler=scaler, \n                                                      device=device, \n                                                      overall_loss=epoch_train_loss.average, \n                                                      overall_metrics=epoch_train_metrics.average,\n                                                      step=passed_steps, \n                                                      epoch=epoch, \n                                                      teacher_model=teacher_model,\n                                                      pseudo_batch=pseudo_batch)\n            \n            lr_key = \"lr\"\n            lr = get_lr(optimizer, only_last=True, key=lr_key)\n            \n            if step % gradient_accumulation_steps == 0:\n                optimization_step(model=model, optimizer=optimizer, scaler=scaler)\n    \n                if scheduling_after == \"step\":\n                    scheduling_step(scheduler, loop=\"training\")\n            \n            elapsed, remain = step_timer(1/1)\n            step_seconds = step_timer.elapsed_time.total_seconds()\n            sample_seconds = step_seconds / batch_size\n            \n            if wandb_run_exists() and \"wandb\" in logger:\n                logs = {\"train/seconds vs step\": step_seconds, \n                        \"train/seconds vs sample\": sample_seconds}\n                \n                wandb.log(logs, step=passed_steps)\n            \n            train_loss.update(batch_loss, n=batch_size)\n            epoch_train_loss.update(batch_loss, n=batch_size)\n            train_metrics.update(batch_metrics, n=batch_size)\n            epoch_train_metrics.update(batch_metrics, n=batch_size)\n            \n            \n            logs = {\"train/loss\": train_loss.average, \n                    \"train/loss vs batch\": batch_loss, \n                    \"train/loss vs epoch\": epoch_train_loss.average,\n                    \"lr\": lr}\n            \n            for metric in batch_metrics:\n                logs.update({f\"train/{metric}\": train_metrics.average[metric], \n                             f\"train/{metric} vs batch\": batch_metrics[metric], \n                             f\"train/{metric} vs epoch\": epoch_train_metrics.average[metric]})\n                \n            if wandb_run_exists() and \"wandb\" in logger:\n                wandb.log(logs, step=passed_steps) \n            \n            if \"tqdm\" in logger:\n                train_loader.set_postfix_str(f\"loss: {epoch_train_loss.average:.{decimals}}\"\n                                             f\"{format_metrics(epoch_train_metrics.average, decimals=decimals)}\")\n            if \"print\" in logger:\n                 if step % verbose == 0 or step == steps and verbose > 0:\n                    elapsed, remain = timer(step/steps)\n                    print(f\"{step}/{steps} - \"\n                          f\"remain: {remain} - \"\n                          f\"loss: {epoch_train_loss.average:.{decimals}}\"\n                          f\"{format_metrics(epoch_train_metrics.average, decimals=decimals)} - \"\n                          f\"lr: {lr}\")\n                    \n            \n            if validation_loader is not None:\n                if (passed_steps % validation_steps) == 0:\n                    if step > validation_steps: print()\n                    validation_loop_steps = len(validation_loader)\n                    validation_batch_size = validation_loader.batch_size\n                    \n                    validation_timer =  Timer(time_format)\n                    validation_loss, validation_metrics, validation_outputs = validation_loop(loader=validation_loader, \n                                                                                              model=model,\n                                                                                              gradient_accumulation_steps=gradient_accumulation_steps,\n                                                                                              amp=amp, \n                                                                                              return_outputs=True, \n                                                                                              verbose=verbose, \n                                                                                              recalculate_metrics_at_end=True, \n                                                                                              device=device, \n                                                                                              logger=logger)\n                    \n                    \n                    elapsed, remain = validation_timer(1/1)\n                    validation_seconds = validation_timer.elapsed_time.total_seconds()\n                    validation_step_seconds = validation_seconds / validation_loop_steps\n                    validation_sample_seconds = validation_step_seconds / validation_batch_size\n            \n                    if wandb_run_exists() and \"wandb\" in logger:\n                        logs = {\"validation/seconds vs step\": validation_step_seconds, \n                                \"validation/seconds vs sample\": validation_sample_seconds}\n                \n                        wandb.log(logs, step=passed_steps)\n                    \n                    \n                    logs = {\"validation/loss\": validation_loss, \n                            \"train/loss vs validation steps\": epoch_train_loss.average}\n    \n                    for metric, value in validation_metrics.items():\n                        logs.update({f\"validation/{metric}\": value, \n                                     f\"train/{metric} vs validation steps\": epoch_train_metrics.average[metric]})\n                    \n                    if wandb_run_exists() and \"wandb\" in logger:\n                        wandb.log(logs, step=passed_steps)\n                    \n                    is_checkpoint_saved = model_checkpointing(loss=validation_loss, \n                                                              metrics=validation_metrics,\n                                                              model=model, \n                                                              optimizer=optimizer, \n                                                              scheduler=scheduler, \n                                                              step=passed_steps, \n                                                              best_loss=best_validation_loss, \n                                                              best_metrics=validation_metrics)\n                    \n                    if is_checkpoint_saved:\n                        best_validation_loss = validation_loss\n                        best_validation_metrics = validation_metrics\n                        best_validation_outputs = validation_outputs\n                        \n                    scheduling_step(scheduler, loss=validation_loss, loop=\"validation\")\n                    print()\n            \n            passed_steps += 1\n        \n        if scheduling_after == \"epoch\":\n            scheduling_step(scheduler, loop=\"training\")\n        \n        on_epoch_end(model=model, \n                     step=passed_steps, \n                     epoch=epoch)\n        \n        if \"tqdm\" in logger and \"print\" not in logger:\n            elapsed, remain = timer(1/1)\n        \n        epoch_elapsed_seconds = timer.elapsed_time.total_seconds()\n        total_time += timedelta(seconds=epoch_elapsed_seconds)\n        \n        if wandb_run_exists() and \"wandb\" in logger:\n            wandb.log({\"epoch\": epoch}, step=passed_steps)\n        \n        if \"tqdm\" in logger: train_loader.close()\n            \n        print(f\"\\nTraining loss: {epoch_train_loss.average:.{decimals}}\"\n              f\"{format_metrics(epoch_train_metrics.average, decimals=decimals)}\")\n        \n        if validation_loader is not None:\n            print(f\"Validation loss: {best_validation_loss:.{decimals}}\"\n                  f\"{format_metrics(best_validation_metrics, decimals=decimals)}\")\n        \n        total_time_string = Timer.format_time(total_time, time_format=time_format)\n        print(f\"Total time: {total_time_string}\")\n    \n    if validation_loader is not None:\n        if return_validation_outputs:\n            return (epoch_train_loss.average, epoch_train_metrics.average), (best_validation_loss, best_validation_metrics, best_validation_outputs)\n        \n        return (epoch_train_loss.average, epoch_train_metrics.average), (best_validation_loss, best_validation_metrics)\n\n    return (epoch_train_loss.average, epoch_train_metrics.average)\n        \ndef validation_loop(loader, \n                    model, \n                    gradient_accumulation_steps=1,\n                    amp=False, \n                    return_outputs=True, \n                    recalculate_metrics_at_end=True, \n                    verbose=1, \n                    device=\"cpu\", \n                    time_format=\"{hours}:{minutes}:{seconds}\",\n                    logger=[\"print\"], \n                    decimals=4):\n    \n    model.eval()\n    loss, metrics = Averager(), Averager()\n    timer = Timer(time_format)\n    outputs, targets = [], []\n    steps = len(loader)\n    \n    if \"tqdm\" in logger:\n        bar_format = \"{l_bar} {bar} {n_fmt}/{total_fmt} - remain: {remaining}{postfix}\"\n        loader = tqdm(iterable=loader, \n                      total=len(loader),\n                      colour=\"#000\",\n                      bar_format=bar_format)\n            \n        loader.set_description_str(\"[Validation]\")\n    \n    is_targets = False\n    for step, batch in enumerate(loader, 1):\n        with torch.no_grad():\n            with autocast(enabled=amp):\n                batch_loss, batch_outputs = calculate_loss(batch=batch, model=model, return_outputs=True, device=device)\n                \n                batch_loss /= gradient_accumulation_steps\n                loss.update(batch_loss.item(), n=len(batch))\n                \n                batch_targets = get_targets(batch)\n                batch_metrics = calculate_metrics(predictions=batch_outputs, targets=batch_targets, device=device)\n                metrics.update(batch_metrics, n=len(batch))\n                \n                if batch_targets is not None:\n                    if isinstance(batch_targets, dict):\n                        targets.append(batch_targets)\n                    else:\n                        targets.extend(batch_targets.to(\"cpu\").tolist())\n                        \n                    is_targets = True\n                \n                outputs.extend(batch_outputs.to(\"cpu\").tolist())\n                \n                if step == steps and recalculate_metrics_at_end and is_targets:\n                    outputs = torch.tensor(outputs)\n                    targets = torch.tensor(targets)\n                        \n                    metrics = Averager(calculate_metrics(predictions=outputs, targets=targets))\n                \n                if \"tqdm\" in logger:\n                    loader.set_postfix_str(f\"loss: {loss.average:.{decimals}}\"\n                                           f\"{format_metrics(metrics.average, decimals=decimals)}\")\n                \n                if \"print\" in logger:\n                    if step % verbose == 0 or step == steps and verbose > 0:\n                        elapsed, remain = timer(step/steps)\n\n                        print(f\"[Validation] \"\n                              f\"{step}/{steps} - \"\n                              f\"remain: {remain} - \"\n                              f\"loss: {loss.average:.{decimals}}\"\n                              f\"{format_metrics(metrics.average, decimals=decimals)}\")\n                    \n    if not recalculate_metrics_at_end: \n        outputs = torch.tensor(outputs)\n        \n    if \"tqdm\" in logger:\n        loader.close()\n        \n    return (loss.average, metrics.average, outputs) if return_outputs else (loss.average, metrics.average)\n\n\ndef format_metrics(metrics, sep=\" - \", add_sep_to_start=True, decimals=4):\n    if metrics != {}:\n        string = sep.join([f\"{k}: {v:.{decimals}}\" for k, v in metrics.items()])\n        return sep + string if add_sep_to_start else string \n    \n    return \"\"\n\n    \ndef training_step(batch, \n                  model, \n                  optimizer, \n                  gradient_norm=1.0, \n                  amp=False, \n                  gradient_accumulation_steps=1, \n                  scaler=None, \n                  device=\"cpu\", \n                  overall_loss=None, \n                  overall_metrics=None, \n                  step=None, \n                  epoch=None,\n                  teacher_model=None,\n                  pseudo_batch=None):\n    \n    model.train()\n    with autocast(enabled=amp):\n        loss, outputs = calculate_loss(batch=batch, model=model, return_outputs=True, device=device)\n        targets = get_targets(batch)\n        metrics = calculate_metrics(predictions=outputs, targets=targets, device=device)\n        \n        loss /= gradient_accumulation_steps\n        loss = backward_step(loss=loss, optimizer=optimizer, scaler=scaler)\n        \n        adversarial_loss = adversarial_step(batch=batch, \n                                            model=model, \n                                            device=device, \n                                            loss=overall_loss, \n                                            metrics=overall_metrics, \n                                            step=step, \n                                            epoch=epoch)\n        \n        if adversarial_loss is not None:\n            adversarial_loss = backward_step(loss=adversarial_loss, optimizer=optimizer, scaler=scaler)\n        \n        if pseudo_batch is not None and teacher_model is not None:\n            pseudo_loss = pseudo_labeling_step(batch=batch,\n                                               pseudo_batch=pseudo_batch,\n                                               model=model, \n                                               teacher_model=teacher_model, \n                                               loss=loss, \n                                               metrics=metrics,\n                                               step=step, \n                                               epoch=epoch, \n                                               device=device)\n        \n            if pseudo_loss is not None:\n                pseudo_loss = backward_step(loss=pseudo_loss, optimizer=optimizer, scaler=scaler)\n            \n    if gradient_norm > 0:\n        if scaler is not None:\n            scaler.unscale_(optimizer)\n                            \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_norm)\n        \n    return loss.detach(), metrics\n\ndef backward_step(loss, optimizer, scaler=None):\n    if scaler is not None:\n        scaler.scale(loss).backward()\n    else:\n        loss.backward()\n        \n    return loss\n        \n\ndef optimization_step(model, optimizer, scaler=None):                        \n    if scaler is not None:\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        optimizer.step()\n        \n    model.zero_grad()\n        \n\ndef scheduling_step(scheduler=None, loss=None, loop=\"training\"):\n    if scheduler is not None:\n        if loop == \"validation\":\n            if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n                scheduler.step(loss)\n        else:\n            if not isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n                scheduler.step()\n\n                \ndef adversarial_step(batch, \n                     model, \n                     device=\"cpu\", \n                     loss=None, \n                     metrics=None, \n                     step=None, \n                     epoch=None):\n    pass\n\n                \n    \ndef calculate_loss(batch, model, return_outputs=True, device=\"cpu\"):\n    raise NotImplementedError(f\"`calculate_loss` function is not implemented.\")\n                \ndef calculate_metrics(predictions, targets, device=\"cpu\"):\n    return dict()\n\ndef get_targets(batch):\n    return []\n\n\ndef on_epoch_end(model=None, step=None, epoch=None):\n    pass\n\n\ndef model_checkpointing(loss, \n                        metrics, \n                        model, \n                        optimizer=None, \n                        scheduler=None, \n                        step=None, \n                        best_loss=None, \n                        best_metrics=None):\n    \n    return True\n\n\ndef pseudo_labeling_step(batch, \n                         pseudo_batch, \n                         model, \n                         teacher_model, \n                         loss=None, \n                         metrics=None, \n                         step=None, \n                         epoch=None, \n                         device=\"cpu\"):\n    pass","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:01.242932Z","iopub.execute_input":"2022-06-20T14:11:01.243477Z","iopub.status.idle":"2022-06-20T14:11:01.311704Z","shell.execute_reply.started":"2022-06-20T14:11:01.243439Z","shell.execute_reply":"2022-06-20T14:11:01.310921Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def calculate_loss(batch, model, return_outputs=True, device=\"cpu\"):\n    input_ids, attention_mask, targets = batch\n    \n    input_ids = input_ids.to(device).long()\n    attention_mask = attention_mask.to(device).long()\n    targets = targets.to(device).float()\n    \n    outputs = model(input_ids, attention_mask)\n    outputs = outputs.sigmoid().squeeze(dim=-1)\n    loss = F.mse_loss(outputs, targets, reduction=\"mean\")\n    \n    return (loss, outputs) if return_outputs else loss\n\n\ndef calculate_metrics(predictions, targets, device=\"cpu\"):\n    predictions = predictions.sigmoid().detach().view(-1).to(\"cpu\").float().numpy()\n    targets = targets.view(-1).to(\"cpu\").float().numpy()\n    \n    return dict(pearson=scipy.stats.pearsonr(predictions, targets)[0])\n\n\ndef get_targets(batch):\n    *_, targets = batch\n    return targets\n\n\ndef model_checkpointing(loss, \n                        metrics, \n                        model, \n                        optimizer=None, \n                        scheduler=None, \n                        step=None, \n                        best_loss=None, \n                        best_metrics=None):\n    \n    is_saved_checkpoint = model_checkpoint(value=metrics[\"pearson\"], \n                                           model=model, \n                                           optimizer=optimizer, \n                                           scheduler=scheduler, \n                                           step=step)\n    return is_saved_checkpoint","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:01.314300Z","iopub.execute_input":"2022-06-20T14:11:01.314923Z","iopub.status.idle":"2022-06-20T14:11:01.326212Z","shell.execute_reply.started":"2022-06-20T14:11:01.314887Z","shell.execute_reply":"2022-06-20T14:11:01.325368Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class DynamicPadding:\n    def __init__(self, tokenizer, max_length=None, padding=True, pad_to_multiple_of=None, return_tensors=\"pt\"):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.padding = padding\n        self.pad_to_multiple_of = pad_to_multiple_of\n        self.return_tensors = return_tensors\n    \n    def __call__(self, tokenized):\n        max_length = max(len(_[\"input_ids\"]) for _ in tokenized)\n        max_length = min(max_length, self.max_length) if self.max_length is not None else max_length\n                \n        padded = self.tokenizer.pad(encoded_inputs=tokenized,\n                                    max_length=max_length,\n                                    padding=self.padding, \n                                    pad_to_multiple_of=self.pad_to_multiple_of, \n                                    return_tensors=self.return_tensors)\n        \n        return padded\n    \n    \n    \nclass Collator:\n    def __init__(self, return_targets=True, **kwargs):\n        self.dynamic_padding = DynamicPadding(**kwargs)\n        self.return_targets = return_targets\n    \n    def __call__(self, batch):\n        all_tokenized, all_targets = [], []\n        for sample in batch:\n            if self.return_targets:\n                tokenized, target = sample\n                all_targets.append(target)\n            else:\n                tokenized = sample\n                \n            all_tokenized.append(tokenized)\n        \n        tokenized = self.dynamic_padding(all_tokenized)\n        \n        input_ids = torch.tensor(tokenized.input_ids)\n        attention_mask = torch.tensor(tokenized.attention_mask)\n        \n        if self.return_targets:\n            all_targets = torch.tensor(all_targets)\n        \n            return input_ids, attention_mask, all_targets\n        \n        return input_ids, attention_mask","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:01.327666Z","iopub.execute_input":"2022-06-20T14:11:01.328169Z","iopub.status.idle":"2022-06-20T14:11:01.340633Z","shell.execute_reply.started":"2022-06-20T14:11:01.328132Z","shell.execute_reply":"2022-06-20T14:11:01.339882Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, texts, pair_texts, tokenizer, contexts=None, sep=None, targets=None, max_length=128):\n        self.texts = texts\n        self.pair_texts = pair_texts\n        self.contexts = contexts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.sep = sep if sep is not None else self.tokenizer.sep_token\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        text = self.texts[index].lower()\n        pair_text = self.pair_texts[index].lower()\n        \n        if self.contexts is not None:\n            context = self.contexts[index].lower()\n            text = text + self.sep + context\n    \n        \n        tokenized = self.tokenizer(text=text, \n                                   text_pair=pair_text, \n                                   add_special_tokens=True,\n                                   #max_length=self.max_length,\n                                   #padding=\"max_length\",\n                                   #truncation=True,\n                                   return_attention_mask=True,\n                                   return_token_type_ids=False,\n                                   return_offsets_mapping=False)\n        \n        \n        if self.targets is not None:\n            target = self.targets[index]\n            \n            return tokenized, target\n            \n        return tokenized","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:01.342003Z","iopub.execute_input":"2022-06-20T14:11:01.342670Z","iopub.status.idle":"2022-06-20T14:11:01.353328Z","shell.execute_reply.started":"2022-06-20T14:11:01.342635Z","shell.execute_reply":"2022-06-20T14:11:01.352646Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, model_path=\"anferico/bert-for-patents\", config_path=None, config_updates={}, reinitialization_layers=0, mixout=0.0):\n        super(Model, self).__init__()\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(model_path)\n        else:\n            self.config = AutoConfig.from_pretrained(config_path)\n        \n        self.config.output_hidden_states = True\n        self.config.update(config_updates)\n        \n        if config_path is None:\n            self.model = AutoModel.from_pretrained(model_path, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        \n        self.model.gradient_checkpointing_enable()\n        print(f\"Gradient Checkpointing: {self.model.is_gradient_checkpointing}\")\n        \n        if mixout > 0:\n            for module in self.model.modules():\n                for name, submodule in module.named_children():\n                    if isinstance(submodule, nn.Dropout):\n                        module.p = 0.0\n                    if isinstance(submodule, nn.Linear):\n                        target_state_dict = submodule.state_dict()\n                        bias = True if submodule.bias is not None else False\n                        \n                        new_module = MixLinear(in_features=submodule.in_features, \n                                               out_features=submodule.out_features, \n                                               bias=bias, \n                                               target=target_state_dict[\"weight\"], \n                                               p=mixout)\n                        \n                        new_module.load_state_dict(target_state_dict)\n                        setattr(module, name, new_module)\n                \n            print(f\"Initialized Mixout (p={mixout}) Regularization\")\n        \n        if reinitialization_layers > 0:\n            layers = ...\n            for layer in layers[-reinitialization_layers:]:\n                for name, module in layer.named_modules():\n                    self.init_weights(module, std=self.config.initializer_range)\n            \n            print(f\"Reinitializated last {n} layers.\")\n\n        self.head = nn.Linear(in_features=self.config.hidden_size, out_features=1)\n        self.init_weights(self.head, std=self.config.initializer_range)\n            \n    \n    def init_weights(self, module, std=0.02):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    \n    def forward(self, input_ids, attention_mask=None):\n        transformer_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        features = transformer_outputs.hidden_states[-1]\n        features = features[:, 0, :]\n        outputs = self.head(features)\n        \n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:01.356727Z","iopub.execute_input":"2022-06-20T14:11:01.357358Z","iopub.status.idle":"2022-06-20T14:11:01.375637Z","shell.execute_reply.started":"2022-06-20T14:11:01.357319Z","shell.execute_reply":"2022-06-20T14:11:01.374913Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"train_path = \"../input/us-patent-phrase-to-phrase-matching/train.csv\"\ntest_path = \"../input/us-patent-phrase-to-phrase-matching/test.csv\"\nsample_submission_path = \"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\"\ncpc_codes_path = \"../input/cpc-codes/titles.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:01.376910Z","iopub.execute_input":"2022-06-20T14:11:01.377683Z","iopub.status.idle":"2022-06-20T14:11:01.387515Z","shell.execute_reply.started":"2022-06-20T14:11:01.377646Z","shell.execute_reply":"2022-06-20T14:11:01.386605Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"cpc_codes = pd.read_csv(cpc_codes_path)\ntrain = pd.read_csv(train_path)\ntrain = train.merge(cpc_codes, left_on=\"context\", right_on=\"code\")\n\nif DEBUG:\n    display(train)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:01.389769Z","iopub.execute_input":"2022-06-20T14:11:01.389971Z","iopub.status.idle":"2022-06-20T14:11:02.284974Z","shell.execute_reply.started":"2022-06-20T14:11:01.389948Z","shell.execute_reply":"2022-06-20T14:11:02.284183Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"                     id        anchor                  target context  score  \\\n0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50   \n1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75   \n2      36d72442aefd8232     abatement         active catalyst     A47   0.25   \n3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50   \n4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00   \n...                 ...           ...                     ...     ...    ...   \n36468  718f1c6953e3942f    undulation     undulatory swimmers     B31   0.00   \n36469  4dc407e6d0aa7844    undulation       voltage fluctuate     B31   0.00   \n36470  de69548ad79caccc  web transfer       transfer from web     B31   0.75   \n36471  6620317413e6e03f  web transfer         transfer to web     B31   0.25   \n36472  96946de83b530746  web transfer            transfer web     B31   0.75   \n\n      code                                              title section  class  \\\n0      A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n1      A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n2      A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n3      A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n4      A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n...    ...                                                ...     ...    ...   \n36468  B31  MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...       B   31.0   \n36469  B31  MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...       B   31.0   \n36470  B31  MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...       B   31.0   \n36471  B31  MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...       B   31.0   \n36472  B31  MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...       B   31.0   \n\n      subclass  group  main_group  \n0          NaN    NaN         NaN  \n1          NaN    NaN         NaN  \n2          NaN    NaN         NaN  \n3          NaN    NaN         NaN  \n4          NaN    NaN         NaN  \n...        ...    ...         ...  \n36468      NaN    NaN         NaN  \n36469      NaN    NaN         NaN  \n36470      NaN    NaN         NaN  \n36471      NaN    NaN         NaN  \n36472      NaN    NaN         NaN  \n\n[36473 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n      <th>code</th>\n      <th>title</th>\n      <th>section</th>\n      <th>class</th>\n      <th>subclass</th>\n      <th>group</th>\n      <th>main_group</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36d72442aefd8232</td>\n      <td>abatement</td>\n      <td>active catalyst</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5296b0c19e1ce60e</td>\n      <td>abatement</td>\n      <td>eliminating process</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54c1e3b9184cb5b6</td>\n      <td>abatement</td>\n      <td>forest region</td>\n      <td>A47</td>\n      <td>0.00</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36468</th>\n      <td>718f1c6953e3942f</td>\n      <td>undulation</td>\n      <td>undulatory swimmers</td>\n      <td>B31</td>\n      <td>0.00</td>\n      <td>B31</td>\n      <td>MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...</td>\n      <td>B</td>\n      <td>31.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36469</th>\n      <td>4dc407e6d0aa7844</td>\n      <td>undulation</td>\n      <td>voltage fluctuate</td>\n      <td>B31</td>\n      <td>0.00</td>\n      <td>B31</td>\n      <td>MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...</td>\n      <td>B</td>\n      <td>31.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36470</th>\n      <td>de69548ad79caccc</td>\n      <td>web transfer</td>\n      <td>transfer from web</td>\n      <td>B31</td>\n      <td>0.75</td>\n      <td>B31</td>\n      <td>MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...</td>\n      <td>B</td>\n      <td>31.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36471</th>\n      <td>6620317413e6e03f</td>\n      <td>web transfer</td>\n      <td>transfer to web</td>\n      <td>B31</td>\n      <td>0.25</td>\n      <td>B31</td>\n      <td>MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...</td>\n      <td>B</td>\n      <td>31.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36472</th>\n      <td>96946de83b530746</td>\n      <td>web transfer</td>\n      <td>transfer web</td>\n      <td>B31</td>\n      <td>0.75</td>\n      <td>B31</td>\n      <td>MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...</td>\n      <td>B</td>\n      <td>31.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>36473 rows Ã— 12 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"cpc_texts = torch.load(\"../input/foldsdump/cpc_texts.pth\")\ntrain['context_text'] = train['context'].map(cpc_texts)\ntrain['text'] = train['anchor'] + '[SEP]' + train['target'] + '[SEP]'  + train['context_text']\ntrain['text'] = train['text'].apply(str.lower)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:02.286231Z","iopub.execute_input":"2022-06-20T14:11:02.286896Z","iopub.status.idle":"2022-06-20T14:11:02.534015Z","shell.execute_reply.started":"2022-06-20T14:11:02.286848Z","shell.execute_reply":"2022-06-20T14:11:02.533301Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:02.535273Z","iopub.execute_input":"2022-06-20T14:11:02.535524Z","iopub.status.idle":"2022-06-20T14:11:02.563933Z","shell.execute_reply.started":"2022-06-20T14:11:02.535490Z","shell.execute_reply":"2022-06-20T14:11:02.563293Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"                     id        anchor                  target context  score  \\\n0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50   \n1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75   \n2      36d72442aefd8232     abatement         active catalyst     A47   0.25   \n3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50   \n4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00   \n...                 ...           ...                     ...     ...    ...   \n36468  718f1c6953e3942f    undulation     undulatory swimmers     B31   0.00   \n36469  4dc407e6d0aa7844    undulation       voltage fluctuate     B31   0.00   \n36470  de69548ad79caccc  web transfer       transfer from web     B31   0.75   \n36471  6620317413e6e03f  web transfer         transfer to web     B31   0.25   \n36472  96946de83b530746  web transfer            transfer web     B31   0.75   \n\n      code                                              title section  class  \\\n0      A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n1      A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n2      A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n3      A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n4      A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n...    ...                                                ...     ...    ...   \n36468  B31  MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...       B   31.0   \n36469  B31  MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...       B   31.0   \n36470  B31  MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...       B   31.0   \n36471  B31  MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...       B   31.0   \n36472  B31  MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...       B   31.0   \n\n      subclass  group  main_group  \\\n0          NaN    NaN         NaN   \n1          NaN    NaN         NaN   \n2          NaN    NaN         NaN   \n3          NaN    NaN         NaN   \n4          NaN    NaN         NaN   \n...        ...    ...         ...   \n36468      NaN    NaN         NaN   \n36469      NaN    NaN         NaN   \n36470      NaN    NaN         NaN   \n36471      NaN    NaN         NaN   \n36472      NaN    NaN         NaN   \n\n                                            context_text  \\\n0      HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n1      HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n2      HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n3      HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n4      HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n...                                                  ...   \n36468  PERFORMING OPERATIONS; TRANSPORTING. MAKING AR...   \n36469  PERFORMING OPERATIONS; TRANSPORTING. MAKING AR...   \n36470  PERFORMING OPERATIONS; TRANSPORTING. MAKING AR...   \n36471  PERFORMING OPERATIONS; TRANSPORTING. MAKING AR...   \n36472  PERFORMING OPERATIONS; TRANSPORTING. MAKING AR...   \n\n                                                    text  \n0      abatement[sep]abatement of pollution[sep]human...  \n1      abatement[sep]act of abating[sep]human necessi...  \n2      abatement[sep]active catalyst[sep]human necess...  \n3      abatement[sep]eliminating process[sep]human ne...  \n4      abatement[sep]forest region[sep]human necessit...  \n...                                                  ...  \n36468  undulation[sep]undulatory swimmers[sep]perform...  \n36469  undulation[sep]voltage fluctuate[sep]performin...  \n36470  web transfer[sep]transfer from web[sep]perform...  \n36471  web transfer[sep]transfer to web[sep]performin...  \n36472  web transfer[sep]transfer web[sep]performing o...  \n\n[36473 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n      <th>code</th>\n      <th>title</th>\n      <th>section</th>\n      <th>class</th>\n      <th>subclass</th>\n      <th>group</th>\n      <th>main_group</th>\n      <th>context_text</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n      <td>abatement[sep]abatement of pollution[sep]human...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n      <td>abatement[sep]act of abating[sep]human necessi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36d72442aefd8232</td>\n      <td>abatement</td>\n      <td>active catalyst</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n      <td>abatement[sep]active catalyst[sep]human necess...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5296b0c19e1ce60e</td>\n      <td>abatement</td>\n      <td>eliminating process</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n      <td>abatement[sep]eliminating process[sep]human ne...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54c1e3b9184cb5b6</td>\n      <td>abatement</td>\n      <td>forest region</td>\n      <td>A47</td>\n      <td>0.00</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n      <td>abatement[sep]forest region[sep]human necessit...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36468</th>\n      <td>718f1c6953e3942f</td>\n      <td>undulation</td>\n      <td>undulatory swimmers</td>\n      <td>B31</td>\n      <td>0.00</td>\n      <td>B31</td>\n      <td>MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...</td>\n      <td>B</td>\n      <td>31.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>PERFORMING OPERATIONS; TRANSPORTING. MAKING AR...</td>\n      <td>undulation[sep]undulatory swimmers[sep]perform...</td>\n    </tr>\n    <tr>\n      <th>36469</th>\n      <td>4dc407e6d0aa7844</td>\n      <td>undulation</td>\n      <td>voltage fluctuate</td>\n      <td>B31</td>\n      <td>0.00</td>\n      <td>B31</td>\n      <td>MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...</td>\n      <td>B</td>\n      <td>31.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>PERFORMING OPERATIONS; TRANSPORTING. MAKING AR...</td>\n      <td>undulation[sep]voltage fluctuate[sep]performin...</td>\n    </tr>\n    <tr>\n      <th>36470</th>\n      <td>de69548ad79caccc</td>\n      <td>web transfer</td>\n      <td>transfer from web</td>\n      <td>B31</td>\n      <td>0.75</td>\n      <td>B31</td>\n      <td>MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...</td>\n      <td>B</td>\n      <td>31.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>PERFORMING OPERATIONS; TRANSPORTING. MAKING AR...</td>\n      <td>web transfer[sep]transfer from web[sep]perform...</td>\n    </tr>\n    <tr>\n      <th>36471</th>\n      <td>6620317413e6e03f</td>\n      <td>web transfer</td>\n      <td>transfer to web</td>\n      <td>B31</td>\n      <td>0.25</td>\n      <td>B31</td>\n      <td>MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...</td>\n      <td>B</td>\n      <td>31.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>PERFORMING OPERATIONS; TRANSPORTING. MAKING AR...</td>\n      <td>web transfer[sep]transfer to web[sep]performin...</td>\n    </tr>\n    <tr>\n      <th>36472</th>\n      <td>96946de83b530746</td>\n      <td>web transfer</td>\n      <td>transfer web</td>\n      <td>B31</td>\n      <td>0.75</td>\n      <td>B31</td>\n      <td>MAKING ARTICLES OF PAPER, CARDBOARD OR MATERIA...</td>\n      <td>B</td>\n      <td>31.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>PERFORMING OPERATIONS; TRANSPORTING. MAKING AR...</td>\n      <td>web transfer[sep]transfer web[sep]performing o...</td>\n    </tr>\n  </tbody>\n</table>\n<p>36473 rows Ã— 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Cross-Validation split","metadata":{}},{"cell_type":"code","source":"train[\"score_bin\"] = pd.cut(train[\"score\"], bins=4, labels=False)\ntrain = create_folds(data_frame=train, \n                     targets=train[\"score_bin\"].values,\n                     groups=train[\"text\"].values,\n                     folds=config.folds, \n                     seed=config.seed, \n                     shuffle=True)\n\nif DEBUG:\n    folds_samples_count = train.groupby(\"fold\").size()\n    display(folds_samples_count)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:02.565116Z","iopub.execute_input":"2022-06-20T14:11:02.565523Z","iopub.status.idle":"2022-06-20T14:11:15.410923Z","shell.execute_reply.started":"2022-06-20T14:11:02.565486Z","shell.execute_reply":"2022-06-20T14:11:15.410147Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"fold\n1    9118\n2    9118\n3    9119\n4    9118\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.model.model_path)\ntokenizer_path = os.path.join(config.output_directory, \"tokenizer/\")\ntokenizer_files = tokenizer.save_pretrained(tokenizer_path)\n\nif DEBUG:\n    print(f\"Tokenizer: {tokenizer}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:15.412503Z","iopub.execute_input":"2022-06-20T14:11:15.412755Z","iopub.status.idle":"2022-06-20T14:11:24.850104Z","shell.execute_reply.started":"2022-06-20T14:11:15.412718Z","shell.execute_reply":"2022-06-20T14:11:24.849327Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Tokenizer: PreTrainedTokenizerFast(name_or_path='anferico/bert-for-patents', vocab_size=39859, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Cross-Validation","metadata":{}},{"cell_type":"code","source":"cv_scores = []\noof_data_frame = pd.DataFrame()\nfor fold in range(1, config.folds + 1):\n    print(f\"Fold {fold}/{config.folds}\", end=\"\\n\"*2)\n    \n    fold_directory = os.path.join(config.output_directory, f\"fold_{fold}\")    \n    make_directory(fold_directory)\n    model_path = os.path.join(fold_directory, \"model.pth\")\n    model_config_path = os.path.join(fold_directory, \"model_config.json\")\n    checkpoints_directory = os.path.join(fold_directory, \"checkpoints/\")\n    make_directory(checkpoints_directory)\n    \n    collator = Collator(tokenizer=tokenizer, max_length=config.max_length)\n    \n    train_fold = train[~train[\"fold\"].isin([fold])]\n    train_dataset = Dataset(texts=train_fold[\"text\"].values, \n                            pair_texts=train_fold[\"target\"].values,\n                            contexts=train_fold[\"title\"].values,\n                            targets=train_fold[\"score\"].values, \n                            max_length=config.max_length,\n                            sep=tokenizer.sep_token,\n                            tokenizer=tokenizer)\n    \n    train_loader = DataLoader(dataset=train_dataset, \n                              batch_size=config.batch_size, \n                              num_workers=config.num_workers,\n                              pin_memory=config.pin_memory,\n                              collate_fn=collator,\n                              shuffle=True, \n                              drop_last=False)\n    \n    print(f\"Train samples: {len(train_dataset)}\")\n    \n    validation_fold = train[train[\"fold\"].isin([fold])]\n    validation_dataset = Dataset(texts=validation_fold[\"text\"].values, \n                                 pair_texts=validation_fold[\"target\"].values,\n                                 contexts=validation_fold[\"title\"].values,\n                                 targets=validation_fold[\"score\"].values,\n                                 max_length=config.max_length,\n                                 sep=tokenizer.sep_token,\n                                 tokenizer=tokenizer)\n    \n    validation_loader = DataLoader(dataset=validation_dataset, \n                                   batch_size=config.batch_size*2, \n                                   num_workers=config.num_workers,\n                                   pin_memory=config.pin_memory,\n                                   collate_fn=collator,\n                                   shuffle=False, \n                                   drop_last=False)\n    \n    print(f\"Validation samples: {len(validation_dataset)}\")\n    \n    \n    model = Model(**config.model)\n    model.config.to_json_file(model_config_path)\n    model_parameters = model.parameters()\n    \n    optimizer = get_optimizer(**config.optimizer, model_parameters=model_parameters)\n    \n    if \"scheduler\" in config:\n        training_steps = len(train_loader) * config.epochs\n        training_steps = int(training_steps // config.gradient_accumulation_steps)\n        \n        config.scheduler.parameters.num_training_steps = training_steps\n        config.scheduler.parameters.num_warmup_steps = training_steps * config.get(\"warmup\", 0)\n        scheduler = transformers.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer=optimizer,num_warmup_steps=training_steps * config.get(\"warmup\", 0),num_training_steps=training_steps,num_cycles=2,last_epoch=-1)\n    else:\n        scheduler = None\n        \n    model_checkpoint = ModelCheckpoint(mode=\"max\", \n                                       delta=config.delta, \n                                       directory=checkpoints_directory, \n                                       overwriting=True, \n                                       filename_format=\"checkpoint.pth\", \n                                       num_candidates=1)\n\n\n    if WANDB: wandb.init(group=EXPERIMENT_NAME, name=f\"Fold {fold}\", config=config)\n    (train_loss, train_metrics), (validation_loss, validation_metrics, validation_outputs) = training_loop(model=model, \n                                                                                                           optimizer=optimizer, \n                                                                                                           scheduler=scheduler,\n                                                                                                           scheduling_after=config.scheduling_after,\n                                                                                                           train_loader=train_loader,\n                                                                                                           validation_loader=validation_loader,\n                                                                                                           epochs=config.epochs, \n                                                                                                           gradient_accumulation_steps=config.gradient_accumulation_steps, \n                                                                                                           gradient_scaling=config.gradient_scaling, \n                                                                                                           gradient_norm=config.gradient_norm, \n                                                                                                           validation_steps=config.validation_steps, \n                                                                                                           amp=config.amp,\n                                                                                                           debug=config.debug, \n                                                                                                           verbose=config.verbose, \n                                                                                                           device=config.device, \n                                                                                                           recalculate_metrics_at_end=True, \n                                                                                                           return_validation_outputs=True, \n                                                                                                           logger=[\"print\", \"wandb\"], \n                                                                                                           decimals=config.decimals)\n    \n    if WANDB: wandb.finish()\n    \n    if config.save_model:\n        model_state = model.state_dict()\n        torch.save(model_state, model_path)\n        print(f\"Model's path: {model_path}\")\n    \n    validation_fold[\"prediction\"] = validation_outputs.to(\"cpu\").numpy()\n    oof_data_frame = pd.concat([oof_data_frame, validation_fold])\n        \n    cv_monitor_value = validation_loss if config.cv_monitor_value == \"loss\" else validation_metrics.get(config.cv_monitor_value, np.nan)\n    cv_scores.append(cv_monitor_value)\n    \n    \n    del model, optimizer, validation_outputs, train_fold, validation_fold\n    torch.cuda.empty_cache()\n    gc.collect()\n    print(end=\"\\n\"*5)\n    \ncv_scores = np.array(cv_scores).round(config.decimals)\nnp.save(\"cv_scores.npy\", cv_scores)\noof_data_frame.to_pickle(\"oof.pkl\")\nconfiguration_path = config.to_json(\"configuration.json\")\n\nprint(f\"CV scores: {cv_scores}\")\nprint(f\"CV mean: {cv_scores.mean():.{config.decimals}}\")\nprint(f\"CV std: {cv_scores.std():.{config.decimals}}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:11:24.851615Z","iopub.execute_input":"2022-06-20T14:11:24.852107Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fold 1/4\n\nTrain samples: 27355\nValidation samples: 9118\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Gradient Checkpointing: True\nEpochs: 5\nAuto Mixed Precision: True\nGradient norm: 1.0\nGradient scaling: True\nGradient accumulation steps: 1\nValidation steps: 200\nDevice: cuda\n\n\nEpoch 1/5\n\n100/342 - remain: 0:12:3 - loss: 0.07094 - pearson: 0.2598 - lr: 5.789473684210527e-06\n200/342 - remain: 0:7:17 - loss: 0.05498 - pearson: 0.4674 - lr: 9.967366355233449e-06\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.02939 - pearson: 0.7584\n'best_value' is improved by inf! New 'best_value': 0.7584057350731968. Checkpoint path: './fold_1/checkpoints/checkpoint.pth'.\n\n300/342 - remain: 0:2:19 - loss: 0.0467 - pearson: 0.5646 - lr: 9.332676413982824e-06\n342/342 - remain: 0:0:0 - loss: 0.04394 - pearson: 0.5914 - lr: 8.843311615125119e-06\n\nTraining loss: 0.04394 - pearson: 0.5914\nValidation loss: 0.02939 - pearson: 0.7584\nTotal time: 0:18:39\n\nEpoch 2/5\n\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.02306 - pearson: 0.8136\n'best_value' is improved by 0.05514547276295756! New 'best_value': 0.8135512078361543. Checkpoint path: './fold_1/checkpoints/checkpoint.pth'.\n\n100/342 - remain: 0:14:57 - loss: 0.02137 - pearson: 0.8286 - lr: 7.257666791554448e-06\n200/342 - remain: 0:8:3 - loss: 0.02114 - pearson: 0.8302 - lr: 5.30091286760834e-06\n\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.02081 - pearson: 0.83\n'best_value' is improved by 0.016455608559827972! New 'best_value': 0.8300068163959823. Checkpoint path: './fold_1/checkpoints/checkpoint.pth'.\n\n300/342 - remain: 0:2:26 - loss: 0.02072 - pearson: 0.8335 - lr: 3.29469570842873e-06\n342/342 - remain: 0:0:0 - loss: 0.0206 - pearson: 0.8343 - lr: 2.517699142189571e-06\n\nTraining loss: 0.0206 - pearson: 0.8343\nValidation loss: 0.02081 - pearson: 0.83\nTotal time: 0:38:13\n\nEpoch 3/5\n\n100/342 - remain: 0:12:1 - loss: 0.01534 - pearson: 0.8763 - lr: 9.985452648855803e-07\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.02015 - pearson: 0.8374\n'best_value' is improved by 0.007361173124982634! New 'best_value': 0.8373679895209649. Checkpoint path: './fold_1/checkpoints/checkpoint.pth'.\n\n200/342 - remain: 0:7:52 - loss: 0.01534 - pearson: 0.8769 - lr: 1.3713958695998898e-07\n300/342 - remain: 0:2:16 - loss: 0.01564 - pearson: 0.8748 - lr: 9.924922379180717e-06\n\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.02172 - pearson: 0.8293\n\n342/342 - remain: 0:0:0 - loss: 0.01572 - pearson: 0.8747 - lr: 9.705405655884796e-06\n\nTraining loss: 0.01572 - pearson: 0.8747\nValidation loss: 0.02015 - pearson: 0.8374\nTotal time: 0:57:36\n\nEpoch 4/5\n\n100/342 - remain: 0:12:18 - loss: 0.01512 - pearson: 0.884 - lr: 8.647357437093106e-06\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.02075 - pearson: 0.8369\n\n200/342 - remain: 0:7:53 - loss: 0.01536 - pearson: 0.8816 - lr: 6.989766565554449e-06\n300/342 - remain: 0:2:15 - loss: 0.01537 - pearson: 0.8809 - lr: 5.00510330102036e-06\n342/342 - remain: 0:0:0 - loss: 0.01523 - pearson: 0.8817 - lr: 4.151869376845695e-06\n\nTraining loss: 0.01523 - pearson: 0.8817\nValidation loss: 0.02015 - pearson: 0.8374\nTotal time: 1:16:2\n\nEpoch 5/5\n\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.01931 - pearson: 0.8467\n'best_value' is improved by 0.009289656599105633! New 'best_value': 0.8466576461200706. Checkpoint path: './fold_1/checkpoints/checkpoint.pth'.\n\n100/342 - remain: 0:14:47 - loss: 0.01051 - pearson: 0.9179 - lr: 2.265259209387867e-06\n200/342 - remain: 0:8:3 - loss: 0.01062 - pearson: 0.9163 - lr: 8.281782631422231e-07\n\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.01905 - pearson: 0.8491\n'best_value' is improved by 0.0024759294170243473! New 'best_value': 0.8491335755370949. Checkpoint path: './fold_1/checkpoints/checkpoint.pth'.\n\n300/342 - remain: 0:2:28 - loss: 0.01066 - pearson: 0.9155 - lr: 7.684997866382548e-08\n342/342 - remain: 0:0:0 - loss: 0.0106 - pearson: 0.9162 - lr: 4.166984667763885e-11\n\nTraining loss: 0.0106 - pearson: 0.9162\nValidation loss: 0.01905 - pearson: 0.8491\nTotal time: 1:35:46\n\n\n\n\n\nFold 2/4\n\nTrain samples: 27355\nValidation samples: 9118\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Gradient Checkpointing: True\nEpochs: 5\nAuto Mixed Precision: True\nGradient norm: 1.0\nGradient scaling: True\nGradient accumulation steps: 1\nValidation steps: 200\nDevice: cuda\n\n\nEpoch 1/5\n\n100/342 - remain: 0:12:25 - loss: 0.06884 - pearson: 0.1908 - lr: 5.789473684210527e-06\n200/342 - remain: 0:7:11 - loss: 0.05657 - pearson: 0.4123 - lr: 9.967366355233449e-06\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.03009 - pearson: 0.7475\n'best_value' is improved by inf! New 'best_value': 0.7475366409727103. Checkpoint path: './fold_2/checkpoints/checkpoint.pth'.\n\n300/342 - remain: 0:2:18 - loss: 0.04755 - pearson: 0.5261 - lr: 9.332676413982824e-06\n342/342 - remain: 0:0:0 - loss: 0.04501 - pearson: 0.5569 - lr: 8.843311615125119e-06\n\nTraining loss: 0.04501 - pearson: 0.5569\nValidation loss: 0.03009 - pearson: 0.7475\nTotal time: 0:18:37\n\nEpoch 2/5\n\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.02307 - pearson: 0.8117\n'best_value' is improved by 0.0641149184052966! New 'best_value': 0.8116515593780069. Checkpoint path: './fold_2/checkpoints/checkpoint.pth'.\n\n100/342 - remain: 0:15:5 - loss: 0.02243 - pearson: 0.8162 - lr: 7.257666791554448e-06\n200/342 - remain: 0:7:57 - loss: 0.02149 - pearson: 0.8249 - lr: 5.30091286760834e-06\n\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.0208 - pearson: 0.8334\n'best_value' is improved by 0.021707175910069654! New 'best_value': 0.8333587352880766. Checkpoint path: './fold_2/checkpoints/checkpoint.pth'.\n\n300/342 - remain: 0:2:28 - loss: 0.02094 - pearson: 0.8299 - lr: 3.29469570842873e-06\n342/342 - remain: 0:0:0 - loss: 0.02071 - pearson: 0.832 - lr: 2.517699142189571e-06\n\nTraining loss: 0.02071 - pearson: 0.832\nValidation loss: 0.0208 - pearson: 0.8334\nTotal time: 0:38:23\n\nEpoch 3/5\n\n100/342 - remain: 0:12:17 - loss: 0.01467 - pearson: 0.8793 - lr: 9.985452648855803e-07\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.02035 - pearson: 0.8394\n'best_value' is improved by 0.006043372672429714! New 'best_value': 0.8394021079605063. Checkpoint path: './fold_2/checkpoints/checkpoint.pth'.\n\n200/342 - remain: 0:8:3 - loss: 0.01535 - pearson: 0.8759 - lr: 1.3713958695998898e-07\n300/342 - remain: 0:2:17 - loss: 0.01551 - pearson: 0.8754 - lr: 9.924922379180717e-06\n\n342/342 - remain: 0:0:0 - loss: 0.0157 - pearson: 0.8744 - lr: 9.705405655884796e-06\n\nTraining loss: 0.0157 - pearson: 0.8744\nValidation loss: 0.02035 - pearson: 0.8394\nTotal time: 0:57:57\n\nEpoch 4/5\n\n100/342 - remain: 0:12:30 - loss: 0.01543 - pearson: 0.8783 - lr: 8.647357437093106e-06\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.0196 - pearson: 0.8425\n'best_value' is improved by 0.0030753805686439817! New 'best_value': 0.8424774885291503. Checkpoint path: './fold_2/checkpoints/checkpoint.pth'.\n\n200/342 - remain: 0:8:7 - loss: 0.0155 - pearson: 0.8782 - lr: 6.989766565554449e-06\n300/342 - remain: 0:2:19 - loss: 0.01533 - pearson: 0.8794 - lr: 5.00510330102036e-06\n342/342 - remain: 0:0:0 - loss: 0.01521 - pearson: 0.8798 - lr: 4.151869376845695e-06\n\nTraining loss: 0.01521 - pearson: 0.8798\nValidation loss: 0.0196 - pearson: 0.8425\nTotal time: 1:16:34\n\nEpoch 5/5\n\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.01982 - pearson: 0.8465\n'best_value' is improved by 0.004042101045617086! New 'best_value': 0.8465195895747674. Checkpoint path: './fold_2/checkpoints/checkpoint.pth'.\n\n100/342 - remain: 0:15:24 - loss: 0.01033 - pearson: 0.918 - lr: 2.265259209387867e-06\n200/342 - remain: 0:8:9 - loss: 0.01067 - pearson: 0.9148 - lr: 8.281782631422231e-07\n\n[Validation] 57/57 - remain: 0:0:0 - loss: 0.01974 - pearson: 0.8493\n'best_value' is improved by 0.0027967011124305774! New 'best_value': 0.8493162906871979. Checkpoint path: './fold_2/checkpoints/checkpoint.pth'.\n\n300/342 - remain: 0:2:28 - loss: 0.01059 - pearson: 0.9161 - lr: 7.684997866382548e-08\n342/342 - remain: 0:0:0 - loss: 0.01052 - pearson: 0.9164 - lr: 4.166984667763885e-11\n\nTraining loss: 0.01052 - pearson: 0.9164\nValidation loss: 0.01974 - pearson: 0.8493\nTotal time: 1:36:27\n\n\n\n\n\nFold 3/4\n\nTrain samples: 27354\nValidation samples: 9119\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Gradient Checkpointing: True\nEpochs: 5\nAuto Mixed Precision: True\nGradient norm: 1.0\nGradient scaling: True\nGradient accumulation steps: 1\nValidation steps: 200\nDevice: cuda\n\n\nEpoch 1/5\n\n100/342 - remain: 0:12:42 - loss: 0.06599 - pearson: 0.2112 - lr: 5.789473684210527e-06\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}